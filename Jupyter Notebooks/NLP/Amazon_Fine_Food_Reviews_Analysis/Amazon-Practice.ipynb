{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"database.sqlite\")\n",
    "\n",
    "df = pd.read_sql_query(\"\"\"SELECT * FROM REVIEWS WHERE Score!=3 LIMIT 5000\"\"\",con)\n",
    "\n",
    "df[\"Score\"] = df[\"Score\"].map(lambda x: 1 if (x==4 or x==5) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A132ETQPMHQ585</th>\n",
       "      <td>1799.0</td>\n",
       "      <td>B001RVFDOO</td>\n",
       "      <td>Donna's Reviews \"I Love Shopping on Amazon!\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.309738e+09</td>\n",
       "      <td>I did not like the taste!</td>\n",
       "      <td>OK...but would never buy them again my husband...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A13E0ARAXI6KJW</th>\n",
       "      <td>2903.0</td>\n",
       "      <td>B000F9Z1WI</td>\n",
       "      <td>Ed Uyeshima</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.156464e+09</td>\n",
       "      <td>Chocolate Wafers in an Identity Crisis as Oreo...</td>\n",
       "      <td>I hate to admit I miss the lard. I was the one...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A13T0V3LHOTHDL</th>\n",
       "      <td>3405.0</td>\n",
       "      <td>B005K4Q1VI</td>\n",
       "      <td>E. Treants \"tree\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.322698e+09</td>\n",
       "      <td>Best Dark Cocoa so fat</td>\n",
       "      <td>Getting really good cocoa out of a K-cup is no...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A14I86SK59DIX</th>\n",
       "      <td>3691.0</td>\n",
       "      <td>B000ER1DIM</td>\n",
       "      <td>ginabi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.327190e+09</td>\n",
       "      <td>organic honey grahams</td>\n",
       "      <td>I bought these for my  grandbabies because the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A15USNEAJUXOSH</th>\n",
       "      <td>435.0</td>\n",
       "      <td>B000G6RYNE</td>\n",
       "      <td>L. Schrank</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.326067e+09</td>\n",
       "      <td>Quite good</td>\n",
       "      <td>I enjoy these chips. I got these instead of my...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVCA516CFZ9HF</th>\n",
       "      <td>103.0</td>\n",
       "      <td>B004K2IHUO</td>\n",
       "      <td>S. Fowler</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335139e+09</td>\n",
       "      <td>TART!</td>\n",
       "      <td>The crust on these tarts are perfect.  My husb...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AXO4PQU0XG3TG</th>\n",
       "      <td>2571.0</td>\n",
       "      <td>B000ED9LDU</td>\n",
       "      <td>Dwight</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.187050e+09</td>\n",
       "      <td>It's bird seed.</td>\n",
       "      <td>I hope that this is really as nutritious as th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AY12DBB0U420B</th>\n",
       "      <td>110.0</td>\n",
       "      <td>B001REEG6C</td>\n",
       "      <td>Gary Peterson</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.316390e+09</td>\n",
       "      <td>My Idea of a Good Diet Food.</td>\n",
       "      <td>I'm presently on a diet and I was at my Fresh ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AY1EF0GOH80EK</th>\n",
       "      <td>530.0</td>\n",
       "      <td>B000G6RYNE</td>\n",
       "      <td>Natasha Stryker</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.327709e+09</td>\n",
       "      <td>Do not miss the salt!</td>\n",
       "      <td>Someone brought these to a party we had last m...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZV26LP92E6WU</th>\n",
       "      <td>1529.0</td>\n",
       "      <td>B0045TDE4Q</td>\n",
       "      <td>M. Hammond \"ColemanFlGuy\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.347149e+09</td>\n",
       "      <td>easy to make also</td>\n",
       "      <td>i followed the easy instructions and these tur...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Id   ProductId  \\\n",
       "UserId                               \n",
       "A132ETQPMHQ585  1799.0  B001RVFDOO   \n",
       "A13E0ARAXI6KJW  2903.0  B000F9Z1WI   \n",
       "A13T0V3LHOTHDL  3405.0  B005K4Q1VI   \n",
       "A14I86SK59DIX   3691.0  B000ER1DIM   \n",
       "A15USNEAJUXOSH   435.0  B000G6RYNE   \n",
       "...                ...         ...   \n",
       "AVCA516CFZ9HF    103.0  B004K2IHUO   \n",
       "AXO4PQU0XG3TG   2571.0  B000ED9LDU   \n",
       "AY12DBB0U420B    110.0  B001REEG6C   \n",
       "AY1EF0GOH80EK    530.0  B000G6RYNE   \n",
       "AZV26LP92E6WU   1529.0  B0045TDE4Q   \n",
       "\n",
       "                                                 ProfileName  \\\n",
       "UserId                                                         \n",
       "A132ETQPMHQ585  Donna's Reviews \"I Love Shopping on Amazon!\"   \n",
       "A13E0ARAXI6KJW                                   Ed Uyeshima   \n",
       "A13T0V3LHOTHDL                             E. Treants \"tree\"   \n",
       "A14I86SK59DIX                                         ginabi   \n",
       "A15USNEAJUXOSH                                    L. Schrank   \n",
       "...                                                      ...   \n",
       "AVCA516CFZ9HF                                      S. Fowler   \n",
       "AXO4PQU0XG3TG                                         Dwight   \n",
       "AY12DBB0U420B                                  Gary Peterson   \n",
       "AY1EF0GOH80EK                                Natasha Stryker   \n",
       "AZV26LP92E6WU                      M. Hammond \"ColemanFlGuy\"   \n",
       "\n",
       "                HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
       "UserId                                                                \n",
       "A132ETQPMHQ585                   0.0                     1.0    0.0   \n",
       "A13E0ARAXI6KJW                   1.0                     1.0    1.0   \n",
       "A13T0V3LHOTHDL                   0.0                     0.0    1.0   \n",
       "A14I86SK59DIX                    0.0                     0.0    1.0   \n",
       "A15USNEAJUXOSH                   0.0                     0.0    1.0   \n",
       "...                              ...                     ...    ...   \n",
       "AVCA516CFZ9HF                    0.0                     0.0    1.0   \n",
       "AXO4PQU0XG3TG                    0.0                     8.0    1.0   \n",
       "AY12DBB0U420B                    0.0                     0.0    1.0   \n",
       "AY1EF0GOH80EK                    3.0                     3.0    1.0   \n",
       "AZV26LP92E6WU                    1.0                     1.0    1.0   \n",
       "\n",
       "                        Time  \\\n",
       "UserId                         \n",
       "A132ETQPMHQ585  1.309738e+09   \n",
       "A13E0ARAXI6KJW  1.156464e+09   \n",
       "A13T0V3LHOTHDL  1.322698e+09   \n",
       "A14I86SK59DIX   1.327190e+09   \n",
       "A15USNEAJUXOSH  1.326067e+09   \n",
       "...                      ...   \n",
       "AVCA516CFZ9HF   1.335139e+09   \n",
       "AXO4PQU0XG3TG   1.187050e+09   \n",
       "AY12DBB0U420B   1.316390e+09   \n",
       "AY1EF0GOH80EK   1.327709e+09   \n",
       "AZV26LP92E6WU   1.347149e+09   \n",
       "\n",
       "                                                          Summary  \\\n",
       "UserId                                                              \n",
       "A132ETQPMHQ585                          I did not like the taste!   \n",
       "A13E0ARAXI6KJW  Chocolate Wafers in an Identity Crisis as Oreo...   \n",
       "A13T0V3LHOTHDL                             Best Dark Cocoa so fat   \n",
       "A14I86SK59DIX                               organic honey grahams   \n",
       "A15USNEAJUXOSH                                         Quite good   \n",
       "...                                                           ...   \n",
       "AVCA516CFZ9HF                                               TART!   \n",
       "AXO4PQU0XG3TG                                     It's bird seed.   \n",
       "AY12DBB0U420B                        My Idea of a Good Diet Food.   \n",
       "AY1EF0GOH80EK                               Do not miss the salt!   \n",
       "AZV26LP92E6WU                                   easy to make also   \n",
       "\n",
       "                                                             Text  test  \n",
       "UserId                                                                   \n",
       "A132ETQPMHQ585  OK...but would never buy them again my husband...   1.0  \n",
       "A13E0ARAXI6KJW  I hate to admit I miss the lard. I was the one...   1.0  \n",
       "A13T0V3LHOTHDL  Getting really good cocoa out of a K-cup is no...   1.0  \n",
       "A14I86SK59DIX   I bought these for my  grandbabies because the...   1.0  \n",
       "A15USNEAJUXOSH  I enjoy these chips. I got these instead of my...   1.0  \n",
       "...                                                           ...   ...  \n",
       "AVCA516CFZ9HF   The crust on these tarts are perfect.  My husb...   1.0  \n",
       "AXO4PQU0XG3TG   I hope that this is really as nutritious as th...   1.0  \n",
       "AY12DBB0U420B   I'm presently on a diet and I was at my Fresh ...   1.0  \n",
       "AY1EF0GOH80EK   Someone brought these to a party we had last m...   1.0  \n",
       "AZV26LP92E6WU   i followed the easy instructions and these tur...   1.0  \n",
       "\n",
       "[139 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### dont run it after the previous cell##\n",
    "\n",
    "d = df[\"UserId\"].value_counts().to_dict()\n",
    "\n",
    "def rem(x):\n",
    "    if d[x]>1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df[\"test\"] = df[\"UserId\"].map(rem)\n",
    "\n",
    "df.loc[:,:] = df[df[\"test\"]==1]\n",
    "\n",
    "gk = df.groupby(\"UserId\")\n",
    "\n",
    "gk.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sort_values(\"ProductId\",axis=0)\n",
    "\n",
    "final = df1.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Why is this $[...] when the same product is available for $[...] here?<br />http://www.amazon.com/VICTOR-FLY-MAGNET-BAIT-REFILL/dp/B00004RBDY<br /><br />The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.',\n",
       "       \"We have used the Victor fly bait for 3 seasons.  Can't beat it.  Great product!\",\n",
       "       \"I just received my shipment and could hardly wait to try this product. We love &quot;slickers&quot; which is what we call them, instead of stickers because they can be removed so easily. My daughter designed signs to be  printed in reverse to use on her car windows. They printed beautifully (we  have 'The Print Shop' program). I am going to have a lot of fun with this  product because there are windows everywhere and other surfaces like tv  screens and computer monitors.\",\n",
       "       ...,\n",
       "       \"This is a bold blend that has a great taste. The flavor comes bursting through. I usually brew & drink Organic Sumatra Mandeling from BJ's. I use this blend exclusively, so to get a cup that rivals the complex flavor from my Tassimo brewer is fantastic. Come on Amazon add it to the subscription service.\",\n",
       "       \"Of all the coffee's available for Tassimo this Kona has the richest flavor and fantastic aroma by<br />far my favorite.\",\n",
       "       \"This coffee supposedly is premium, it tastes watery--very thin.  Not good at all.  Maybe old??<br />Not sure, it was a waste, and I'm using it line the bottom of my sitting shoes and trash cans and rained-on luggage, to absorb the smells.  So I used it, but not to drink.  Do not buy.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[\"Text\"] # will give a series type where each row value for text column will be printed\n",
    "final[\"Text\"].values ## will give only arrray of the req column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurisation- Bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "## we will get a final matrix with all vi's for all ri's\n",
    "set_of_vectors_matrix = count_vec.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4986x13510 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 253517 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_of_vectors_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"sample\":count_vec.get_feature_names()})\n",
    "#df.to_excel(\"1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4986, 13510)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(set_of_vectors_matrix))\n",
    "set_of_vectors_matrix.shape\n",
    "\n",
    "## columns are very huge as it follows BoW concept and will create a d-dim vector by taking \n",
    "## each word in the data corpus as single dimension\n",
    "\n",
    "## then it will give the count value for every word in the ri and forms vi, a sparse matrix \n",
    "## finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chaitanyamajety/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasti\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "# nltk -> natural language processing tool kit\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords ## to locate the frequently occuring words in the data corpus\n",
    "from nltk.stem import PorterStemmer ## to find the root words for a given tensed word\n",
    "from nltk.stem import WordNetLemmatizer ## to identify words which cant be seperated by\n",
    "# changing the actual meaning of the word :eg: NewYork\n",
    "\n",
    "stop = (set(stopwords.words(\"english\")))\n",
    "snow = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "def cleanhtml(sentence):\n",
    "    cleantext = re.sub(r\"<.*?>\",r\"\",sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence):\n",
    "    cleantext = re.sub(r'[\"|\\'|?|!|#]',r\"\",sentence)\n",
    "    cleantext = re.sub(r'[\\|/|.|,|(|)]',r\" \",sentence)\n",
    "    return cleantext\n",
    "\n",
    "print(snow.stem(\"tasty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "all_pos_words = []\n",
    "all_neg_words = []\n",
    "final_text = []\n",
    "s=\"\"\n",
    "\n",
    "for sent in final[\"Text\"].values:\n",
    "    final_word=[]\n",
    "    cleantext = cleanhtml(sent)\n",
    "    for word in cleantext.split(): # cleantext = chaitanya#boy\n",
    "        for fw in cleanpunc(word).split():# cleanpunc(word).split() = [chaitanya,boy]\n",
    "            if(fw.isalpha() and len(fw)>2):\n",
    "                if(fw.lower() not in stop):\n",
    "                    s = (snow.stem(fw.lower())).encode(\"utf8\")\n",
    "                    final_word.append(s)\n",
    "                    if((final[\"Score\"].values)[i] == 1):\n",
    "                        all_pos_words.append(s)\n",
    "                    elif((final[\"Score\"].values)[i] == 0):\n",
    "                        all_neg_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(final_word)\n",
    "    final_text.append(str1)\n",
    "    i+=1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>2774</td>\n",
       "      <td>B00002NCJC</td>\n",
       "      <td>A196AJHU9EASJN</td>\n",
       "      <td>Alex Chaffee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1282953600</td>\n",
       "      <td>thirty bucks?</td>\n",
       "      <td>Why is this $[...] when the same product is av...</td>\n",
       "      <td>b'product avail www amazon com victor trap unr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>2775</td>\n",
       "      <td>B00002NCJC</td>\n",
       "      <td>A13RRPGE79XFFH</td>\n",
       "      <td>reader48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1281052800</td>\n",
       "      <td>Flies Begone</td>\n",
       "      <td>We have used the Victor fly bait for 3 seasons...</td>\n",
       "      <td>b'use victor fli bait season beat great'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>1244</td>\n",
       "      <td>B00002Z754</td>\n",
       "      <td>A3B8RCEI0FXFI6</td>\n",
       "      <td>B G Chase</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>962236800</td>\n",
       "      <td>WOW Make your own 'slickers' !</td>\n",
       "      <td>I just received my shipment and could hardly w...</td>\n",
       "      <td>b'receiv shipment could hard wait tri product ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1245</td>\n",
       "      <td>B00002Z754</td>\n",
       "      <td>A29Z5PI9BW2PU3</td>\n",
       "      <td>Robbie</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>961718400</td>\n",
       "      <td>Great Product</td>\n",
       "      <td>This was a really good idea and the final prod...</td>\n",
       "      <td>b'realli good idea final product outstand use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>3204</td>\n",
       "      <td>B000084DVR</td>\n",
       "      <td>A1UGDJP1ZJWVPF</td>\n",
       "      <td>T. Moore \"thoughtful reader\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1177977600</td>\n",
       "      <td>Good stuff!</td>\n",
       "      <td>I'm glad my 45lb cocker/standard poodle puppy ...</td>\n",
       "      <td>b'glad cocker standard poodl puppi love stuff ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id   ProductId          UserId                   ProfileName  \\\n",
       "2546  2774  B00002NCJC  A196AJHU9EASJN                  Alex Chaffee   \n",
       "2547  2775  B00002NCJC  A13RRPGE79XFFH                      reader48   \n",
       "1145  1244  B00002Z754  A3B8RCEI0FXFI6                     B G Chase   \n",
       "1146  1245  B00002Z754  A29Z5PI9BW2PU3                        Robbie   \n",
       "2942  3204  B000084DVR  A1UGDJP1ZJWVPF  T. Moore \"thoughtful reader\"   \n",
       "\n",
       "      HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "2546                     0                       0      1  1282953600   \n",
       "2547                     0                       0      1  1281052800   \n",
       "1145                    10                      10      1   962236800   \n",
       "1146                     7                       7      1   961718400   \n",
       "2942                     1                       1      1  1177977600   \n",
       "\n",
       "                             Summary  \\\n",
       "2546                   thirty bucks?   \n",
       "2547                    Flies Begone   \n",
       "1145  WOW Make your own 'slickers' !   \n",
       "1146                   Great Product   \n",
       "2942                     Good stuff!   \n",
       "\n",
       "                                                   Text  \\\n",
       "2546  Why is this $[...] when the same product is av...   \n",
       "2547  We have used the Victor fly bait for 3 seasons...   \n",
       "1145  I just received my shipment and could hardly w...   \n",
       "1146  This was a really good idea and the final prod...   \n",
       "2942  I'm glad my 45lb cocker/standard poodle puppy ...   \n",
       "\n",
       "                                             Clean_text  \n",
       "2546  b'product avail www amazon com victor trap unr...  \n",
       "2547           b'use victor fli bait season beat great'  \n",
       "1145  b'receiv shipment could hard wait tri product ...  \n",
       "1146  b'realli good idea final product outstand use ...  \n",
       "2942  b'glad cocker standard poodl puppi love stuff ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.loc[:,\"Clean_text\"] = final_text\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"final.sqlite\")\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql(\"Reviews\",conn,if_exists=\"replace\",schema=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams and N grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(b'like', 1804), (b'tast', 1613), (b'good', 1511), (b'flavor', 1509), (b'love', 1453), (b'great', 1367), (b'use', 1267), (b'one', 1166), (b'product', 1148), (b'tri', 1146), (b'coffe', 996), (b'food', 987), (b'make', 980), (b'chip', 962), (b'get', 827), (b'tea', 779), (b'bag', 745), (b'buy', 715), (b'eat', 699), (b'realli', 696)]\n",
      "[(b'like', 443), (b'tast', 422), (b'product', 391), (b'tri', 282), (b'one', 278), (b'flavor', 260), (b'would', 247), (b'food', 233), (b'use', 226), (b'good', 198), (b'buy', 185), (b'order', 182), (b'tea', 180), (b'get', 179), (b'bag', 177), (b'chip', 177), (b'even', 168), (b'make', 161), (b'box', 155), (b'mix', 153)]\n"
     ]
    }
   ],
   "source": [
    "freq_pos_words = nltk.FreqDist(all_pos_words)\n",
    "freq_neg_words = nltk.FreqDist(all_neg_words)\n",
    "print(freq_pos_words.most_common(20))\n",
    "print(freq_neg_words.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2)) # both 1gram and 2grams will be taken\n",
    "## if (1,4) then until 4 grams can be taken\n",
    "final_bigram_counts = count_vect.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 148211)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigram_counts.shape ## Here the column size is increased as we have discussed already\n",
    "## if gram size increases then dimensionality also increases\n",
    "## here it increased drastically from 13510(unigram) -> 148211(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vec_mat = tfidf.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 148211)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec_mat.shape\n",
    "\n",
    "## it will be same as for Bag of words because here also the sparse matrix formation is same\n",
    "## only the value in each dimension of the vector will change in TFIDF theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148211\n"
     ]
    }
   ],
   "source": [
    "features = tfidf.get_feature_names()\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec_mat[1].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fly bait</td>\n",
       "      <td>0.274736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seasons can</td>\n",
       "      <td>0.274736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for seasons</td>\n",
       "      <td>0.274736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>victor</td>\n",
       "      <td>0.262108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bait for</td>\n",
       "      <td>0.262108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the victor</td>\n",
       "      <td>0.262108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>victor fly</td>\n",
       "      <td>0.262108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fly</td>\n",
       "      <td>0.246199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>seasons</td>\n",
       "      <td>0.246199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bait</td>\n",
       "      <td>0.240521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>beat it</td>\n",
       "      <td>0.235720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>can beat</td>\n",
       "      <td>0.201504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>used the</td>\n",
       "      <td>0.192545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>have used</td>\n",
       "      <td>0.186498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>beat</td>\n",
       "      <td>0.173265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>it great</td>\n",
       "      <td>0.169864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>great product</td>\n",
       "      <td>0.154497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>we have</td>\n",
       "      <td>0.142644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>used</td>\n",
       "      <td>0.109802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>we</td>\n",
       "      <td>0.092118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>product</td>\n",
       "      <td>0.080693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>can</td>\n",
       "      <td>0.077881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>great</td>\n",
       "      <td>0.074089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>have</td>\n",
       "      <td>0.061395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>for</td>\n",
       "      <td>0.051863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term     tfidf\n",
       "0        fly bait  0.274736\n",
       "1     seasons can  0.274736\n",
       "2     for seasons  0.274736\n",
       "3          victor  0.262108\n",
       "4        bait for  0.262108\n",
       "5      the victor  0.262108\n",
       "6      victor fly  0.262108\n",
       "7             fly  0.246199\n",
       "8         seasons  0.246199\n",
       "9            bait  0.240521\n",
       "10        beat it  0.235720\n",
       "11       can beat  0.201504\n",
       "12       used the  0.192545\n",
       "13      have used  0.186498\n",
       "14           beat  0.173265\n",
       "15       it great  0.169864\n",
       "16  great product  0.154497\n",
       "17        we have  0.142644\n",
       "18           used  0.109802\n",
       "19             we  0.092118\n",
       "20        product  0.080693\n",
       "21            can  0.077881\n",
       "22          great  0.074089\n",
       "23           have  0.061395\n",
       "24            for  0.051863"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_25_feats(arr,feats,n):\n",
    "    top_ids = np.argsort(arr)[::-1][0:n]\n",
    "    top_feats = []\n",
    "    for i in top_ids:\n",
    "        top_feats.append((feats[i],arr[i]))\n",
    "    feat_df = pd.DataFrame(top_feats,columns=[\"term\",\"tfidf\"])\n",
    "    return feat_df\n",
    "\n",
    "top_tdfidf = top_25_feats(tfidf_vec_mat[1].toarray()[0],features,25)\n",
    "top_tdfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Word2Vec model design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why is this $[...] when the same product is available for $[...] here?<br />http://www.amazon.com/VICTOR-FLY-MAGNET-BAIT-REFILL/dp/B00004RBDY<br /><br />The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.\n",
      "******************************************************************************************\n",
      "['why', 'is', 'this', 'when', 'the', 'same', 'product', 'is', 'available', 'for', 'www', 'amazon', 'com', 'dp', 'victor', 'and', 'traps', 'are', 'unreal', 'of', 'course', 'total', 'fly', 'genocide', 'pretty', 'stinky', 'but', 'only', 'right', 'nearby']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "list_of_sent = []\n",
    "\n",
    "for sent in final[\"Text\"].values:\n",
    "    filtered_words = []\n",
    "    for word in cleanhtml(sent).split():\n",
    "        for cleaned_word in cleanpunc(word).split():\n",
    "            if(cleaned_word.isalpha()):\n",
    "                filtered_words.append(cleaned_word.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sent.append(filtered_words)\n",
    "            \n",
    "print(final[\"Text\"].values[0]) # original sentence\n",
    "print(\"*\"*90)\n",
    "print(list_of_sent[0]) # words filtered from the sentence as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(list_of_sent,min_count=5,vector_size=50,workers=4)\n",
    "\n",
    "# min_count is for checking to only take if the word is available for 5 times atleast\n",
    "# each word will be converted to a dimension mentioned in vector size \n",
    "# Workers will be helpful to use specific number of cores on the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.9476836919784546),\n",
       " ('flavorful', 0.9089567065238953),\n",
       " ('light', 0.9014713168144226),\n",
       " ('fluffy', 0.8962560296058655),\n",
       " ('crunchy', 0.8796468377113342),\n",
       " ('soft', 0.8771519064903259),\n",
       " ('crisp', 0.8678852319717407),\n",
       " ('fresh', 0.8662703037261963),\n",
       " ('thin', 0.8583195805549622),\n",
       " ('healthy', 0.8547101616859436)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"tasty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg-W2V and Weighted Avg-TDIDF-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4986\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "## Avg - W2V computation\n",
    "\n",
    "sent_vectors = []\n",
    "\n",
    "for sent in list_of_sent:\n",
    "    word_vec = np.zeros(50)\n",
    "    word_count = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            x = w2v_model.wv[word]\n",
    "            word_vec += x\n",
    "            word_count += 1            \n",
    "        except:\n",
    "            continue\n",
    "    word_vec/=word_count\n",
    "    sent_vectors.append(word_vec)\n",
    "    \n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "w2v_model.wv[\"like\"] # every word matrix is a dense matrix in W2V technique\n",
    "print(tfidf_vec_mat[0].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4986\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6afb44fc04ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_tfidf_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_tfidf_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "tf_idf_feat = tfidf.get_feature_names()\n",
    "\n",
    "avg_tfidf_vec = []\n",
    "row = 0\n",
    "for sent in list_of_sent:\n",
    "    tfidf_vec = np.array(50)\n",
    "    weighted_sum = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            tfidf_val = tfidf_vec_mat[row,tf_idf_feat.index(word)]  \n",
    "            tfidf_vec += (vec*tfidf_val)\n",
    "            weighted_sum += tfidf_val\n",
    "        except:\n",
    "            continue\n",
    "    tfidf_vec = np.true_divide(tfidf_vec,weighted_sum)\n",
    "    row+=1\n",
    "    avg_tfidf_vec.append(tfidf_vec)\n",
    "\n",
    "print(len(avg_tfidf_vec))\n",
    "print(len(avg_tfidf_vec[0]))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
