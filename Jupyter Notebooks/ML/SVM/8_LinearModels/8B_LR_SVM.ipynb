{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ArWK463kbhL",
    "outputId": "ad250ffe-29ed-4dc9-bf30-fe91ab10656c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mldzJdakbhS"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('task_b.csv')\n",
    "data=data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsCrC2wckbhV",
    "outputId": "fff03fba-880e-4875-9bba-f05797f08d1d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-195.871045</td>\n",
       "      <td>-14843.084171</td>\n",
       "      <td>5.532140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1217.183964</td>\n",
       "      <td>-4068.124621</td>\n",
       "      <td>4.416082</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.138451</td>\n",
       "      <td>4413.412028</td>\n",
       "      <td>0.425317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>363.824242</td>\n",
       "      <td>15474.760647</td>\n",
       "      <td>1.094119</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-768.812047</td>\n",
       "      <td>-7963.932192</td>\n",
       "      <td>1.870536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1            f2        f3    y\n",
       "0  -195.871045 -14843.084171  5.532140  1.0\n",
       "1 -1217.183964  -4068.124621  4.416082  1.0\n",
       "2     9.138451   4413.412028  0.425317  0.0\n",
       "3   363.824242  15474.760647  1.094119  0.0\n",
       "4  -768.812047  -7963.932192  1.870536  0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FI18joJ_kbhZ",
    "outputId": "22e420e9-4295-4307-a60f-1a528d07c81d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1    0.067172\n",
       "f2   -0.017944\n",
       "f3    0.839060\n",
       "y     1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u40oCVMikbhc",
    "outputId": "db6dce7e-7469-4aa5-8af3-1c08cd0f0081",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1      488.195035\n",
       "f2    10403.417325\n",
       "f3        2.926662\n",
       "y         0.501255\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQIbNaHskbhe",
    "outputId": "f2298482-b1d5-47e0-f15c-31f4a753a9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "X=data[['f1','f2','f3']].values\n",
    "Y=data['y'].values\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUxp9-qEkbhh"
   },
   "source": [
    "# What if our features are with different variance \n",
    "\n",
    "<pre>\n",
    "* <b>As part of this task you will observe how linear models work in case of data having feautres with different variance</b>\n",
    "* <b>from the output of the above cells you can observe that var(F2)>>var(F1)>>Var(F3)</b>\n",
    "\n",
    "> <b>Task1</b>:\n",
    "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n",
    "    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n",
    "\n",
    "> <b>Task2</b>:\n",
    "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n",
    "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
    "    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n",
    "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbMnsrxakbhi"
   },
   "source": [
    "<h3><font color='blue'> Make sure you write the observations for each task, why a particular feautre got more importance than others</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1 - Feature importance using Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr = linear_model.SGDClassifier(loss='log',penalty='l2',alpha=0.0001,eta0=0.0001,random_state=15,verbose=2,learning_rate=\"constant\")\n",
    "clf_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.08, NNZs: 3, Bias: -0.001751, T: 200, Avg. loss: 2516.147588\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.001551, T: 400, Avg. loss: 2621.694380\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.001850, T: 600, Avg. loss: 3285.222158\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.003527, T: 800, Avg. loss: 3142.216822\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.004027, T: 1000, Avg. loss: 3009.886714\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.40, NNZs: 3, Bias: -0.003523, T: 1200, Avg. loss: 3032.001946\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 6 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.37170471, -1.34463853,  0.12669033]]), array([-0.00352309]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr.coef_,clf_lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**<br>\n",
    "*In a Over all view, while computing W_T*xi, W1 will play major role in deciding the output as its having highest weight and if xi1 is negative then it will tend to be more positive class and if xi1 is positive then product will be more negative leading to a negative class*<br>\n",
    "*1.As in Weights array, W1 is negative it will increase the probability of being output be negative(class 0) if xi is positive*<br>\n",
    "*2.Remaining two weights W0 and W2 will increase the probability of being output be positive(class 1) if xi is positive, Here as W0 is more dominant , it will decide the output class to be 1 or 0 to the maximum extent*<br>\n",
    "*3.Alternatively, W1 is negative it will increase the probability of being output be positive(class 0) if xi is negative*<br>\n",
    "*4.Similarly, Remaining two weights W0 and W2 will increase the probability of being output be negative(class 0) if xi is negative, Here as W0 is more dominant , it will decide the output class to be 1 or 0 to the maximum extent*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "      <th>y_tilda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-768.812047</td>\n",
       "      <td>-7963.932192</td>\n",
       "      <td>1.870536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>192.093461</td>\n",
       "      <td>-12677.139687</td>\n",
       "      <td>3.456229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-31.315888</td>\n",
       "      <td>-15289.241646</td>\n",
       "      <td>3.882981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>355.555296</td>\n",
       "      <td>-16420.510738</td>\n",
       "      <td>1.968104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-371.562551</td>\n",
       "      <td>-2334.218517</td>\n",
       "      <td>2.231601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-157.066935</td>\n",
       "      <td>12149.075908</td>\n",
       "      <td>6.944223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>841.946716</td>\n",
       "      <td>-3756.578733</td>\n",
       "      <td>0.430296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>119.423142</td>\n",
       "      <td>-2985.720392</td>\n",
       "      <td>0.929967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>443.199825</td>\n",
       "      <td>-1053.252455</td>\n",
       "      <td>5.467800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-51.189253</td>\n",
       "      <td>7442.423346</td>\n",
       "      <td>9.528478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1            f2        f3    y  y_tilda\n",
       "4   -768.812047  -7963.932192  1.870536  0.0        1\n",
       "5    192.093461 -12677.139687  3.456229  0.0        1\n",
       "8    -31.315888 -15289.241646  3.882981  0.0        1\n",
       "10   355.555296 -16420.510738  1.968104  0.0        1\n",
       "11  -371.562551  -2334.218517  2.231601  0.0        1\n",
       "..          ...           ...       ...  ...      ...\n",
       "191 -157.066935  12149.075908  6.944223  1.0        0\n",
       "192  841.946716  -3756.578733  0.430296  0.0        1\n",
       "195  119.423142  -2985.720392  0.929967  0.0        1\n",
       "198  443.199825  -1053.252455  5.467800  0.0        1\n",
       "199  -51.189253   7442.423346  9.528478  1.0        0\n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = clf_lr.coef_\n",
    "y_tilda = []\n",
    "for i in range(len(data)):\n",
    "    temp = np.dot(w[0],np.array(data.iloc[i,0:3]))\n",
    "    if(temp<0):\n",
    "        y_tilda.append(0)\n",
    "    elif(temp>0):\n",
    "        y_tilda.append(1)\n",
    "df = data.copy()\n",
    "df[\"y_tilda\"]=y_tilda\n",
    "df[df[\"y_tilda\"]!=df[\"y\"]]\n",
    "# so here we ended up with wrong output for 95 values with the optimised weight vector\n",
    "# Also this is due to non-standardization of feature data and we are wrongly predicting\n",
    "## nearly 50% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task1 - Feature importance using SVM(hinge loss)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = linear_model.SGDClassifier(loss='hinge',penalty='l2',alpha=0.0001,eta0=0.0001,random_state=15,verbose=2,learning_rate=\"constant\")\n",
    "clf_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.001600, T: 200, Avg. loss: 2634.084615\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.001100, T: 400, Avg. loss: 2593.136418\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000900, T: 600, Avg. loss: 3308.216351\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.002700, T: 800, Avg. loss: 3155.085896\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.93, NNZs: 3, Bias: -0.002800, T: 1000, Avg. loss: 3080.501847\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.43, NNZs: 3, Bias: -0.002700, T: 1200, Avg. loss: 3011.887174\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.002200, T: 1400, Avg. loss: 3002.132514\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 7 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.38249139, -0.55764501,  0.15407861]]), array([-0.0022]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.coef_,clf_svm.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**<br>\n",
    "*In a Over all view, while computing W_T*xi, W1 will play major role in deciding the output as its having highest weight and if xi1 is negative then it will tend to be more positive class and if xi1 is positive then product will be more negative leading to a negative class*<br>\n",
    "*1.As in Weights array, W1 is negative it will increase the probability of being output be negative(class 0) if xi is positive*<br>\n",
    "*2.Remaining two weights W0 and W2 will increase the probability of being output be positive(class 1) if xi is positive, Here as W0 is more dominant , it will decide the output class to be 1 or 0 to the maximum extent*<br>\n",
    "*3.Alternatively, W1 is negative it will increase the probability of being output be positive(class 0) if xi is negative*<br>\n",
    "*4.Similarly, Remaining two weights W0 and W2 will increase the probability of being output be negative(class 0) if xi is negative, Here as W0 is more dominant , it will decide the output class to be 1 or 0 to the maximum extent*<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "      <th>y_tilda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-768.812047</td>\n",
       "      <td>-7963.932192</td>\n",
       "      <td>1.870536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>192.093461</td>\n",
       "      <td>-12677.139687</td>\n",
       "      <td>3.456229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-31.315888</td>\n",
       "      <td>-15289.241646</td>\n",
       "      <td>3.882981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>355.555296</td>\n",
       "      <td>-16420.510738</td>\n",
       "      <td>1.968104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-371.562551</td>\n",
       "      <td>-2334.218517</td>\n",
       "      <td>2.231601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-157.066935</td>\n",
       "      <td>12149.075908</td>\n",
       "      <td>6.944223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>841.946716</td>\n",
       "      <td>-3756.578733</td>\n",
       "      <td>0.430296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>119.423142</td>\n",
       "      <td>-2985.720392</td>\n",
       "      <td>0.929967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>443.199825</td>\n",
       "      <td>-1053.252455</td>\n",
       "      <td>5.467800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-51.189253</td>\n",
       "      <td>7442.423346</td>\n",
       "      <td>9.528478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1            f2        f3    y  y_tilda\n",
       "4   -768.812047  -7963.932192  1.870536  0.0        1\n",
       "5    192.093461 -12677.139687  3.456229  0.0        1\n",
       "8    -31.315888 -15289.241646  3.882981  0.0        1\n",
       "10   355.555296 -16420.510738  1.968104  0.0        1\n",
       "11  -371.562551  -2334.218517  2.231601  0.0        1\n",
       "..          ...           ...       ...  ...      ...\n",
       "191 -157.066935  12149.075908  6.944223  1.0        0\n",
       "192  841.946716  -3756.578733  0.430296  0.0        1\n",
       "195  119.423142  -2985.720392  0.929967  0.0        1\n",
       "198  443.199825  -1053.252455  5.467800  0.0        1\n",
       "199  -51.189253   7442.423346  9.528478  1.0        0\n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = clf_svm.coef_\n",
    "y_tilda = []\n",
    "for i in range(len(data)):\n",
    "    temp = np.dot(w[0],np.array(data.iloc[i,0:3]))\n",
    "    if(temp<0):\n",
    "        y_tilda.append(0)\n",
    "    elif(temp>0):\n",
    "        y_tilda.append(1)\n",
    "df_svm = data.copy()\n",
    "df_svm[\"y_tilda\"]=y_tilda\n",
    "df_svm[df_svm[\"y_tilda\"]!=df_svm[\"y\"]]\n",
    "# so here we ended up with wrong output for 95 values with the optimised weight vector\n",
    "# Also this is due to non-standardization of feature data and we are wrongly predicting\n",
    "## nearly 50% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Feature importance using Logistic Regression after standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Standaradising the features data as they are having high varaince and output prediction \n",
    "## is going wrong\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_std = linear_model.SGDClassifier(loss='log',penalty='l2',alpha=0.0001,eta0=0.0001,random_state=15,verbose=2,learning_rate=\"constant\")\n",
    "clf_lr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 3, Bias: 0.000001, T: 200, Avg. loss: 0.691431\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 3, Bias: 0.000002, T: 400, Avg. loss: 0.687922\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.03, NNZs: 3, Bias: 0.000002, T: 600, Avg. loss: 0.684449\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 3, Bias: 0.000003, T: 800, Avg. loss: 0.681011\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.04, NNZs: 3, Bias: 0.000003, T: 1000, Avg. loss: 0.677608\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.05, NNZs: 3, Bias: 0.000003, T: 1200, Avg. loss: 0.674240\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.06, NNZs: 3, Bias: 0.000003, T: 1400, Avg. loss: 0.670905\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000003, T: 1600, Avg. loss: 0.667605\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000002, T: 1800, Avg. loss: 0.664338\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000002, T: 2000, Avg. loss: 0.661104\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000002, T: 2200, Avg. loss: 0.657903\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.10, NNZs: 3, Bias: 0.000001, T: 2400, Avg. loss: 0.654734\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2600, Avg. loss: 0.651598\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2800, Avg. loss: 0.648493\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.12, NNZs: 3, Bias: 0.000001, T: 3000, Avg. loss: 0.645419\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.13, NNZs: 3, Bias: 0.000002, T: 3200, Avg. loss: 0.642377\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.14, NNZs: 3, Bias: 0.000002, T: 3400, Avg. loss: 0.639365\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.14, NNZs: 3, Bias: 0.000001, T: 3600, Avg. loss: 0.636383\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.15, NNZs: 3, Bias: 0.000001, T: 3800, Avg. loss: 0.633431\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.16, NNZs: 3, Bias: 0.000001, T: 4000, Avg. loss: 0.630509\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.17, NNZs: 3, Bias: 0.000001, T: 4200, Avg. loss: 0.627616\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4400, Avg. loss: 0.624752\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4600, Avg. loss: 0.621917\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.19, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.619110\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 5000, Avg. loss: 0.616331\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5200, Avg. loss: 0.613579\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5400, Avg. loss: 0.610855\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.000001, T: 5600, Avg. loss: 0.608158\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.23, NNZs: 3, Bias: -0.000002, T: 5800, Avg. loss: 0.605488\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.23, NNZs: 3, Bias: -0.000003, T: 6000, Avg. loss: 0.602845\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.24, NNZs: 3, Bias: -0.000002, T: 6200, Avg. loss: 0.600227\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.25, NNZs: 3, Bias: -0.000002, T: 6400, Avg. loss: 0.597635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6600, Avg. loss: 0.595069\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6800, Avg. loss: 0.592528\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.27, NNZs: 3, Bias: -0.000003, T: 7000, Avg. loss: 0.590011\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7200, Avg. loss: 0.587519\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7400, Avg. loss: 0.585052\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.29, NNZs: 3, Bias: -0.000004, T: 7600, Avg. loss: 0.582608\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 7800, Avg. loss: 0.580189\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 8000, Avg. loss: 0.577793\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.31, NNZs: 3, Bias: -0.000007, T: 8200, Avg. loss: 0.575420\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.32, NNZs: 3, Bias: -0.000008, T: 8400, Avg. loss: 0.573071\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.33, NNZs: 3, Bias: -0.000009, T: 8600, Avg. loss: 0.570743\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.33, NNZs: 3, Bias: -0.000010, T: 8800, Avg. loss: 0.568439\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.000010, T: 9000, Avg. loss: 0.566156\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.000010, T: 9200, Avg. loss: 0.563896\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.000012, T: 9400, Avg. loss: 0.561657\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.36, NNZs: 3, Bias: -0.000012, T: 9600, Avg. loss: 0.559439\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.37, NNZs: 3, Bias: -0.000013, T: 9800, Avg. loss: 0.557243\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.37, NNZs: 3, Bias: -0.000015, T: 10000, Avg. loss: 0.555067\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.38, NNZs: 3, Bias: -0.000016, T: 10200, Avg. loss: 0.552912\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000017, T: 10400, Avg. loss: 0.550777\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000018, T: 10600, Avg. loss: 0.548663\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.40, NNZs: 3, Bias: -0.000020, T: 10800, Avg. loss: 0.546568\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.40, NNZs: 3, Bias: -0.000021, T: 11000, Avg. loss: 0.544493\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.41, NNZs: 3, Bias: -0.000023, T: 11200, Avg. loss: 0.542438\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.42, NNZs: 3, Bias: -0.000024, T: 11400, Avg. loss: 0.540402\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.42, NNZs: 3, Bias: -0.000025, T: 11600, Avg. loss: 0.538384\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.43, NNZs: 3, Bias: -0.000027, T: 11800, Avg. loss: 0.536386\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.000028, T: 12000, Avg. loss: 0.534406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.000030, T: 12200, Avg. loss: 0.532444\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.000030, T: 12400, Avg. loss: 0.530501\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.000032, T: 12600, Avg. loss: 0.528575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.46, NNZs: 3, Bias: -0.000034, T: 12800, Avg. loss: 0.526668\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000035, T: 13000, Avg. loss: 0.524777\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000038, T: 13200, Avg. loss: 0.522904\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.000039, T: 13400, Avg. loss: 0.521048\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000042, T: 13600, Avg. loss: 0.519209\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000046, T: 13800, Avg. loss: 0.517387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.50, NNZs: 3, Bias: -0.000049, T: 14000, Avg. loss: 0.515581\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.50, NNZs: 3, Bias: -0.000054, T: 14200, Avg. loss: 0.513791\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.51, NNZs: 3, Bias: -0.000057, T: 14400, Avg. loss: 0.512018\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000060, T: 14600, Avg. loss: 0.510260\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000064, T: 14800, Avg. loss: 0.508518\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.000067, T: 15000, Avg. loss: 0.506792\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.000069, T: 15200, Avg. loss: 0.505081\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000072, T: 15400, Avg. loss: 0.503385\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000076, T: 15600, Avg. loss: 0.501705\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.55, NNZs: 3, Bias: -0.000080, T: 15800, Avg. loss: 0.500039\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000084, T: 16000, Avg. loss: 0.498387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000088, T: 16200, Avg. loss: 0.496751\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000091, T: 16400, Avg. loss: 0.495128\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000095, T: 16600, Avg. loss: 0.493520\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.58, NNZs: 3, Bias: -0.000100, T: 16800, Avg. loss: 0.491925\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 0.58, NNZs: 3, Bias: -0.000104, T: 17000, Avg. loss: 0.490345\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.000108, T: 17200, Avg. loss: 0.488778\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000113, T: 17400, Avg. loss: 0.487225\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000118, T: 17600, Avg. loss: 0.485685\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.000123, T: 17800, Avg. loss: 0.484158\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.000128, T: 18000, Avg. loss: 0.482644\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 0.62, NNZs: 3, Bias: -0.000132, T: 18200, Avg. loss: 0.481144\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 0.62, NNZs: 3, Bias: -0.000137, T: 18400, Avg. loss: 0.479656\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.000143, T: 18600, Avg. loss: 0.478180\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.000147, T: 18800, Avg. loss: 0.476718\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.000153, T: 19000, Avg. loss: 0.475267\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.000158, T: 19200, Avg. loss: 0.473829\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 0.65, NNZs: 3, Bias: -0.000164, T: 19400, Avg. loss: 0.472402\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 0.65, NNZs: 3, Bias: -0.000170, T: 19600, Avg. loss: 0.470988\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 0.66, NNZs: 3, Bias: -0.000176, T: 19800, Avg. loss: 0.469585\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 0.67, NNZs: 3, Bias: -0.000182, T: 20000, Avg. loss: 0.468194\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 0.67, NNZs: 3, Bias: -0.000188, T: 20200, Avg. loss: 0.466815\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.000194, T: 20400, Avg. loss: 0.465447\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.000201, T: 20600, Avg. loss: 0.464090\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.000207, T: 20800, Avg. loss: 0.462744\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.000215, T: 21000, Avg. loss: 0.461410\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 0.70, NNZs: 3, Bias: -0.000222, T: 21200, Avg. loss: 0.460086\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 0.70, NNZs: 3, Bias: -0.000228, T: 21400, Avg. loss: 0.458773\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.000235, T: 21600, Avg. loss: 0.457471\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.000241, T: 21800, Avg. loss: 0.456179\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.000249, T: 22000, Avg. loss: 0.454898\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.000255, T: 22200, Avg. loss: 0.453627\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 0.73, NNZs: 3, Bias: -0.000262, T: 22400, Avg. loss: 0.452366\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 0.73, NNZs: 3, Bias: -0.000269, T: 22600, Avg. loss: 0.451115\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.000277, T: 22800, Avg. loss: 0.449874\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.000284, T: 23000, Avg. loss: 0.448643\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.000292, T: 23200, Avg. loss: 0.447422\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.000300, T: 23400, Avg. loss: 0.446210\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000308, T: 23600, Avg. loss: 0.445008\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000316, T: 23800, Avg. loss: 0.443816\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.000326, T: 24000, Avg. loss: 0.442632\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.000334, T: 24200, Avg. loss: 0.441458\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 0.78, NNZs: 3, Bias: -0.000343, T: 24400, Avg. loss: 0.440293\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 0.78, NNZs: 3, Bias: -0.000352, T: 24600, Avg. loss: 0.439138\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000362, T: 24800, Avg. loss: 0.437991\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000370, T: 25000, Avg. loss: 0.436852\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000378, T: 25200, Avg. loss: 0.435723\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 0.80, NNZs: 3, Bias: -0.000388, T: 25400, Avg. loss: 0.434602\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 0.80, NNZs: 3, Bias: -0.000397, T: 25600, Avg. loss: 0.433490\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 0.81, NNZs: 3, Bias: -0.000407, T: 25800, Avg. loss: 0.432387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 0.81, NNZs: 3, Bias: -0.000417, T: 26000, Avg. loss: 0.431291\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.000426, T: 26200, Avg. loss: 0.430204\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.000436, T: 26400, Avg. loss: 0.429125\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 0.83, NNZs: 3, Bias: -0.000446, T: 26600, Avg. loss: 0.428055\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 0.83, NNZs: 3, Bias: -0.000457, T: 26800, Avg. loss: 0.426992\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.000467, T: 27000, Avg. loss: 0.425937\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.000478, T: 27200, Avg. loss: 0.424891\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000490, T: 27400, Avg. loss: 0.423851\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000501, T: 27600, Avg. loss: 0.422820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000512, T: 27800, Avg. loss: 0.421796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.000523, T: 28000, Avg. loss: 0.420780\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.000534, T: 28200, Avg. loss: 0.419771\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.000546, T: 28400, Avg. loss: 0.418770\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.000556, T: 28600, Avg. loss: 0.417776\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.000568, T: 28800, Avg. loss: 0.416789\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.000579, T: 29000, Avg. loss: 0.415809\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.000591, T: 29200, Avg. loss: 0.414836\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.000603, T: 29400, Avg. loss: 0.413871\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 147 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_std.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03851242, -0.00553122,  0.88963322]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lr_std.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**<br>\n",
    "*1. Here we can clearly see that the weights completely changed after standardising the features as because f1 is having more high range of values causing the weight also to make high*<br>\n",
    "*2. So Ideally W2 is more important in determining the output label, because for a positive xi, it will make more positive and for a negative xi it will tend to more negative class*<br>\n",
    "*3. W0 is having next highest importance and behaves in the same way as W2*<br>\n",
    "*4. W1 is having less weight and if xi is negative it will be of very less movement to be positive, in the same way if xi is positive it will slightly tend towards negative class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           f1        f2        f3    y  y_tilda\n",
      "1   -2.520394 -0.517290 -0.200648  1.0        0\n",
      "15   0.943057  0.438012  0.065379  0.0        1\n",
      "28   0.517737 -0.214087 -0.035179  1.0        0\n",
      "31   0.348143 -0.447289  0.180963  0.0        1\n",
      "36   0.461469 -0.482392 -0.139486  1.0        0\n",
      "40   2.300804  0.249974  0.084692  0.0        1\n",
      "50   1.600541 -0.063459 -0.232022  1.0        0\n",
      "74  -1.308833  0.147053 -0.035927  1.0        0\n",
      "95  -0.394592  0.809318  0.177856  0.0        1\n",
      "96  -0.818095  0.021554 -0.020103  1.0        0\n",
      "145  1.402883 -0.914822 -0.012546  0.0        1\n",
      "153 -0.658206  1.640986 -0.142281  1.0        0\n",
      "159  1.940247  1.368369  0.069814  0.0        1\n",
      "162  2.168675  1.094113  0.253738  0.0        1\n",
      "164  1.722800  1.153099 -0.244297  1.0        0\n",
      "181  1.594267 -0.729767  0.241171  0.0        1\n",
      "187  1.056864  1.320062  0.031328  0.0        1\n",
      "189 -1.311197 -0.426307 -0.173383  1.0        0\n",
      "196 -0.098538  1.133361  0.143667  0.0        1\n",
      "198  0.889207 -0.226766  0.159612  0.0        1 \n",
      " (20, 5)\n"
     ]
    }
   ],
   "source": [
    "w = clf_lr_std.coef_\n",
    "y_tilda = []\n",
    "for i in range(len(X)):\n",
    "    temp = np.dot(w[0],np.array(X[i]))# dot product of weight vector and standardised feats\n",
    "    if(temp<0):\n",
    "        y_tilda.append(0)\n",
    "    elif(temp>0):\n",
    "        y_tilda.append(1)\n",
    "# converting \"data\" data frame into standaradised format to make reading visible\n",
    "df_lr_std = pd.DataFrame(X,columns=[\"f1\",\"f2\",\"f3\"])\n",
    "df_lr_std[\"y\"] = Y\n",
    "df_lr_std[\"y_tilda\"]=y_tilda\n",
    "print(df_lr_std[df_lr_std[\"y_tilda\"]!=df_lr_std[\"y\"]],\"\\n\",\n",
    "      df_lr_std[df_lr_std[\"y_tilda\"]!=df_lr_std[\"y\"]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**<br>\n",
    "*1.Now only 20 points are misclassified after standardisation process*<br>\n",
    "*2.So as mentioned above the more absolute value of weight will cause the impact on the output label either for positive class or negative class depends on the polarity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Feature importance using SVM(hinge loss) after standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_std = linear_model.SGDClassifier(loss='hinge',penalty='l2',alpha=0.0001,eta0=0.0001,random_state=15,verbose=2,learning_rate=\"constant\")\n",
    "clf_svm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.02, NNZs: 3, Bias: 0.000000, T: 200, Avg. loss: 0.993111\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.03, NNZs: 3, Bias: -0.000000, T: 400, Avg. loss: 0.978934\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.05, NNZs: 3, Bias: 0.000000, T: 600, Avg. loss: 0.964757\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 800, Avg. loss: 0.950580\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.08, NNZs: 3, Bias: -0.000000, T: 1000, Avg. loss: 0.936403\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.10, NNZs: 3, Bias: 0.000000, T: 1200, Avg. loss: 0.922226\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.12, NNZs: 3, Bias: 0.000000, T: 1400, Avg. loss: 0.908049\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.13, NNZs: 3, Bias: 0.000000, T: 1600, Avg. loss: 0.893873\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.15, NNZs: 3, Bias: 0.000000, T: 1800, Avg. loss: 0.879696\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.17, NNZs: 3, Bias: 0.000000, T: 2000, Avg. loss: 0.865519\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.19, NNZs: 3, Bias: -0.000000, T: 2200, Avg. loss: 0.851342\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 2400, Avg. loss: 0.837165\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.000000, T: 2600, Avg. loss: 0.822988\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.24, NNZs: 3, Bias: -0.000000, T: 2800, Avg. loss: 0.808812\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.25, NNZs: 3, Bias: 0.000000, T: 3000, Avg. loss: 0.794635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.27, NNZs: 3, Bias: 0.000000, T: 3200, Avg. loss: 0.780458\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.29, NNZs: 3, Bias: -0.000000, T: 3400, Avg. loss: 0.766282\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.30, NNZs: 3, Bias: 0.000000, T: 3600, Avg. loss: 0.752105\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.32, NNZs: 3, Bias: 0.000000, T: 3800, Avg. loss: 0.737929\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.000000, T: 4000, Avg. loss: 0.723752\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.35, NNZs: 3, Bias: 0.000000, T: 4200, Avg. loss: 0.709575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.37, NNZs: 3, Bias: 0.000000, T: 4400, Avg. loss: 0.695399\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000000, T: 4600, Avg. loss: 0.681222\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.40, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.667046\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.42, NNZs: 3, Bias: 0.000000, T: 5000, Avg. loss: 0.652870\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.44, NNZs: 3, Bias: 0.000000, T: 5200, Avg. loss: 0.638693\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.45, NNZs: 3, Bias: 0.000000, T: 5400, Avg. loss: 0.624517\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000000, T: 5600, Avg. loss: 0.610341\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000000, T: 5800, Avg. loss: 0.596164\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.51, NNZs: 3, Bias: -0.000000, T: 6000, Avg. loss: 0.581988\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000000, T: 6200, Avg. loss: 0.567812\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000000, T: 6400, Avg. loss: 0.553635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000100, T: 6600, Avg. loss: 0.539559\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000400, T: 6800, Avg. loss: 0.525831\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.000400, T: 7000, Avg. loss: 0.512822\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000300, T: 7200, Avg. loss: 0.500814\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.62, NNZs: 3, Bias: 0.000100, T: 7400, Avg. loss: 0.489867\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.63, NNZs: 3, Bias: 0.000500, T: 7600, Avg. loss: 0.479843\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.64, NNZs: 3, Bias: 0.000900, T: 7800, Avg. loss: 0.470023\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.66, NNZs: 3, Bias: 0.001200, T: 8000, Avg. loss: 0.460939\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.67, NNZs: 3, Bias: 0.001100, T: 8200, Avg. loss: 0.453123\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.68, NNZs: 3, Bias: 0.001000, T: 8400, Avg. loss: 0.446506\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.69, NNZs: 3, Bias: 0.001000, T: 8600, Avg. loss: 0.440244\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.70, NNZs: 3, Bias: 0.001000, T: 8800, Avg. loss: 0.434183\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.71, NNZs: 3, Bias: 0.001200, T: 9000, Avg. loss: 0.428457\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.72, NNZs: 3, Bias: 0.001400, T: 9200, Avg. loss: 0.422915\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.73, NNZs: 3, Bias: 0.001500, T: 9400, Avg. loss: 0.417423\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.74, NNZs: 3, Bias: 0.001800, T: 9600, Avg. loss: 0.412336\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.75, NNZs: 3, Bias: 0.001900, T: 9800, Avg. loss: 0.407610\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.76, NNZs: 3, Bias: 0.002200, T: 10000, Avg. loss: 0.403083\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.77, NNZs: 3, Bias: 0.002500, T: 10200, Avg. loss: 0.398796\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.78, NNZs: 3, Bias: 0.002900, T: 10400, Avg. loss: 0.394692\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.79, NNZs: 3, Bias: 0.003400, T: 10600, Avg. loss: 0.390702\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.80, NNZs: 3, Bias: 0.004000, T: 10800, Avg. loss: 0.386766\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.81, NNZs: 3, Bias: 0.004800, T: 11000, Avg. loss: 0.383045\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.81, NNZs: 3, Bias: 0.005600, T: 11200, Avg. loss: 0.379523\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.82, NNZs: 3, Bias: 0.006400, T: 11400, Avg. loss: 0.376092\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.83, NNZs: 3, Bias: 0.006900, T: 11600, Avg. loss: 0.372853\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.84, NNZs: 3, Bias: 0.007400, T: 11800, Avg. loss: 0.369757\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.85, NNZs: 3, Bias: 0.007900, T: 12000, Avg. loss: 0.366660\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.85, NNZs: 3, Bias: 0.008500, T: 12200, Avg. loss: 0.363659\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.86, NNZs: 3, Bias: 0.008900, T: 12400, Avg. loss: 0.360918\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.87, NNZs: 3, Bias: 0.009300, T: 12600, Avg. loss: 0.358293\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.87, NNZs: 3, Bias: 0.009700, T: 12800, Avg. loss: 0.355741\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.88, NNZs: 3, Bias: 0.010200, T: 13000, Avg. loss: 0.353291\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.89, NNZs: 3, Bias: 0.010500, T: 13200, Avg. loss: 0.350947\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.89, NNZs: 3, Bias: 0.010800, T: 13400, Avg. loss: 0.348714\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.90, NNZs: 3, Bias: 0.011000, T: 13600, Avg. loss: 0.346524\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 13800, Avg. loss: 0.344410\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 14000, Avg. loss: 0.342392\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.92, NNZs: 3, Bias: 0.010900, T: 14200, Avg. loss: 0.340372\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.93, NNZs: 3, Bias: 0.010800, T: 14400, Avg. loss: 0.338400\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.93, NNZs: 3, Bias: 0.010600, T: 14600, Avg. loss: 0.336501\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 14800, Avg. loss: 0.334604\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 15000, Avg. loss: 0.332782\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.95, NNZs: 3, Bias: 0.010300, T: 15200, Avg. loss: 0.331037\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.96, NNZs: 3, Bias: 0.010200, T: 15400, Avg. loss: 0.329317\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 0.96, NNZs: 3, Bias: 0.010100, T: 15600, Avg. loss: 0.327597\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 15800, Avg. loss: 0.325932\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 16000, Avg. loss: 0.324369\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16200, Avg. loss: 0.322840\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16400, Avg. loss: 0.321310\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.99, NNZs: 3, Bias: 0.010000, T: 16600, Avg. loss: 0.319872\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.99, NNZs: 3, Bias: 0.010100, T: 16800, Avg. loss: 0.318513\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1.00, NNZs: 3, Bias: 0.010300, T: 17000, Avg. loss: 0.317175\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1.00, NNZs: 3, Bias: 0.010500, T: 17200, Avg. loss: 0.315862\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1.01, NNZs: 3, Bias: 0.010700, T: 17400, Avg. loss: 0.314549\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1.01, NNZs: 3, Bias: 0.010900, T: 17600, Avg. loss: 0.313237\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1.02, NNZs: 3, Bias: 0.011100, T: 17800, Avg. loss: 0.311924\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1.02, NNZs: 3, Bias: 0.011300, T: 18000, Avg. loss: 0.310612\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1.03, NNZs: 3, Bias: 0.011500, T: 18200, Avg. loss: 0.309337\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1.03, NNZs: 3, Bias: 0.011700, T: 18400, Avg. loss: 0.308166\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1.04, NNZs: 3, Bias: 0.011900, T: 18600, Avg. loss: 0.307057\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1.04, NNZs: 3, Bias: 0.012200, T: 18800, Avg. loss: 0.305980\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1.05, NNZs: 3, Bias: 0.012500, T: 19000, Avg. loss: 0.304914\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1.05, NNZs: 3, Bias: 0.012800, T: 19200, Avg. loss: 0.303880\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1.06, NNZs: 3, Bias: 0.013100, T: 19400, Avg. loss: 0.302892\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1.06, NNZs: 3, Bias: 0.013500, T: 19600, Avg. loss: 0.301927\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.013900, T: 19800, Avg. loss: 0.300979\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.014100, T: 20000, Avg. loss: 0.300054\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.014300, T: 20200, Avg. loss: 0.299188\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 101 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', random_state=15, verbose=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_std.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04248904, 0.02585179, 1.07272982]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_std.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**<br>\n",
    "*1. Here we can clearly see that the weights completely changed after standardising the features as because f1 is having more high range of values(in the input data csv -high variance) causing the weight also to make high*<br>\n",
    "*2. So Ideally W2 is more important in determining the output label, because for a positive xi, it will make more positive and for a negative xi it will tend to more negative class*<br>\n",
    "*3. W0 is having next highest importance and behaves in the same way as W2*<br>\n",
    "*4. W1 is having less weight and if xi is negative it will be of very less movement to be negative, in the same way if xi is positive it will slightly tend towards positive class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           f1        f2        f3    y  y_tilda\n",
      "1   -2.520394 -0.517290 -0.200648  1.0        0\n",
      "15   0.943057  0.438012  0.065379  0.0        1\n",
      "28   0.517737 -0.214087 -0.035179  1.0        0\n",
      "31   0.348143 -0.447289  0.180963  0.0        1\n",
      "36   0.461469 -0.482392 -0.139486  1.0        0\n",
      "40   2.300804  0.249974  0.084692  0.0        1\n",
      "50   1.600541 -0.063459 -0.232022  1.0        0\n",
      "74  -1.308833  0.147053 -0.035927  1.0        0\n",
      "95  -0.394592  0.809318  0.177856  0.0        1\n",
      "96  -0.818095  0.021554 -0.020103  1.0        0\n",
      "145  1.402883 -0.914822 -0.012546  0.0        1\n",
      "153 -0.658206  1.640986 -0.142281  1.0        0\n",
      "159  1.940247  1.368369  0.069814  0.0        1\n",
      "162  2.168675  1.094113  0.253738  0.0        1\n",
      "164  1.722800  1.153099 -0.244297  1.0        0\n",
      "181  1.594267 -0.729767  0.241171  0.0        1\n",
      "187  1.056864  1.320062  0.031328  0.0        1\n",
      "189 -1.311197 -0.426307 -0.173383  1.0        0\n",
      "196 -0.098538  1.133361  0.143667  0.0        1\n",
      "198  0.889207 -0.226766  0.159612  0.0        1 \n",
      " (20, 5)\n"
     ]
    }
   ],
   "source": [
    "w = clf_svm_std.coef_\n",
    "y_tilda = []\n",
    "for i in range(len(X)):\n",
    "    temp = np.dot(w[0],np.array(X[i])) # dot product of weight vector and standardised feats\n",
    "    if(temp<0):\n",
    "        y_tilda.append(0)\n",
    "    elif(temp>0):\n",
    "        y_tilda.append(1)\n",
    "# converting \"data\" data frame into standaradised format to make reading visible\n",
    "df_svm_std = pd.DataFrame(X,columns=[\"f1\",\"f2\",\"f3\"])\n",
    "df_svm_std[\"y\"] = Y\n",
    "df_svm_std[\"y_tilda\"]=y_tilda\n",
    "print(df_svm_std[df_svm_std[\"y_tilda\"]!=df_svm_std[\"y\"]],\"\\n\",\n",
    "      df_svm_std[df_svm_std[\"y_tilda\"]!=df_svm_std[\"y\"]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**<br>\n",
    "*1.Now only 20 points are misclassified after standardisation process and 180 points are correctly classified now*<br>\n",
    "*2.So as mentioned above the more absolute value of weight will cause the impact on the output label either for positive class or negative class depends on the polarity*<br>\n",
    "*3. As Linear SVM is almost equivalent to Logistic regression, we are getting mostly the same outputs even thought weight vector have subtle difference in magnitude*<br>\n",
    "*4. The most important thing is if standardisation is not done, we may end up in treating a unimportant feature as an important feature due to its high variance factor as seen before*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "8B_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
