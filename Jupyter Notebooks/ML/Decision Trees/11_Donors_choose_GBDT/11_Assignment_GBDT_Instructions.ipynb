{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wqapAXjsc24"
   },
   "source": [
    "# Assignment 9: GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8u5NhbnEsc2-"
   },
   "source": [
    "#### Response Coding: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIFWmwHKsc3A"
   },
   "source": [
    "<img src='http://i.imgur.com/TufZptV.jpg' width=700px>\n",
    "\n",
    "> The response tabel is built only on train dataset.\n",
    "> For a category which is not there in train data and present in test data, we will encode them with default values\n",
    "Ex: in our test data if have State: D then we encode it as [0.5, 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3KwfGufDsc3D"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply GBDT on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical(instead of one hot encoding, try <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/'>response coding</a>: use probability values), numerical features + project_title(TFIDF)+  preprocessed_eassay (TFIDF)+sentiment Score of eassay(check the bellow example, include all 4 values as 4 features)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical(instead of one hot encoding, try <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/handling-categorical-and-numerical-features/'>response coding</a>: use probability values), numerical features + project_title(TFIDF W2V)+  preprocessed_eassay (TFIDF W2V)</li>        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning (Consider any two hyper parameters)</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation/simple cross validation data</li>\n",
    "    <li>use gridsearch cv or randomsearch cv or you can write your own for loops to do this task</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>n_estimators</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n",
    "            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>n_estimators</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n",
    "    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "            </ul>\n",
    "    <br>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iffa2_WGsc3H",
    "outputId": "151fc2d5-3bc1-4206-9bd2-000dbf662b31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\installed\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning:\n",
      "\n",
      "The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: 0.01, neu: 0.745, pos: 0.245, compound: 0.9975, "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for_sentiment = 'a person is a person no matter how small dr seuss i teach the smallest students with the biggest enthusiasm \\\n",
    "for learning my students learn in many different ways using all of our senses and multiple intelligences i use a wide range\\\n",
    "of techniques to help all my students succeed students in my class come from a variety of different backgrounds which makes\\\n",
    "for wonderful sharing of experiences and cultures including native americans our school is a caring community of successful \\\n",
    "learners which can be seen through collaborative student project based learning in and out of the classroom kindergarteners \\\n",
    "in my class love to work with hands on materials and have many different opportunities to practice a skill before it is\\\n",
    "mastered having the social skills to work cooperatively with friends is a crucial aspect of the kindergarten curriculum\\\n",
    "montana is the perfect place to learn about agriculture and nutrition my students love to role play in our pretend kitchen\\\n",
    "in the early childhood classroom i have had several kids ask me can we try cooking with real food i will take their idea \\\n",
    "and create common core cooking lessons where we learn important math and writing concepts while cooking delicious healthy \\\n",
    "food for snack time my students will have a grounded appreciation for the work that went into making the food and knowledge \\\n",
    "of where the ingredients came from as well as how it is healthy for their bodies this project would expand our learning of \\\n",
    "nutrition and agricultural cooking recipes by having us peel our own apples to make homemade applesauce make our own bread \\\n",
    "and mix up healthy plants from our classroom garden in the spring we will also create our own cookbooks to be printed and \\\n",
    "shared with families students will gain math and literature skills as well as a life long enjoyment for healthy cooking \\\n",
    "nannan'\n",
    "ss = sid.polarity_scores(for_sentiment)\n",
    "\n",
    "for k in ss:\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "\n",
    "# we can use these 4 things as features/attributes (neg, neu, pos, compound)\n",
    "# neg: 0.0, neu: 0.753, pos: 0.247, compound: 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOyTl7oKsc3W"
   },
   "source": [
    "<h1>1. GBDT (xgboost/lightgbm) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWge-b2Zsc3Z"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDZFu6-usc3d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "X = data.drop(\"project_is_approved\",axis=1)\n",
    "Y = data[\"project_is_approved\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lAMFKFcsc3j"
   },
   "source": [
    "<h2>1.2 Splitting data into Train and cross validation(or test): Stratified Sampling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODMijuuNsc3l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verfication after the splitting th data: 109248\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,random_state=15,test_size=0.33,stratify=Y)\n",
    "\n",
    "print(\"Verfication after the splitting th data:\",x_train.shape[0]+x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsALgl5Asc3u"
   },
   "source": [
    "<h2>1.3 Make Data Model Ready: encoding essay</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A_85jbWsc3v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After TFIDF vectorization for feature: essay\n",
      "(73196, 5000) (73196,)\n",
      "(36052, 5000) (36052,)\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73196/73196 [01:42<00:00, 712.66it/s]\n",
      "100%|██████████| 36052/36052 [00:50<00:00, 720.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After TFIDF-W2V vectorization for feature: essay\n",
      "(73196, 300) (73196,)\n",
      "(36052, 300) (36052,)\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "\n",
    "## preparing set 1 data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10,max_features=5000,ngram_range=(1,4))\n",
    "x_train_essay_tfidf = vectorizer.fit_transform(x_train[\"essay\"].values)\n",
    "x_test_essay_tfidf = vectorizer.transform(x_test[\"essay\"].values)\n",
    "\n",
    "print(\"After TFIDF vectorization for feature: essay\")\n",
    "print(x_train_essay_tfidf.shape,y_train.shape)\n",
    "print(x_test_essay_tfidf.shape,y_test.shape)\n",
    "print(\"*\"*50)\n",
    "\n",
    "## preparing for set 2 data\n",
    "## computing IDF Values and formming a dictionary of it for every unique value\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "tfidfvectorizer.fit(x_train[\"essay\"].values)\n",
    "# forming a dictionary such that {feature_name:idf_value} ==> will be helpful later\n",
    "# zip function will take only iterable objects\n",
    "dictionary = dict(zip(tfidfvectorizer.get_feature_names(),list(tfidfvectorizer.idf_)))\n",
    "tfidf_words = set(tfidfvectorizer.get_feature_names())\n",
    "\n",
    "from tqdm import tqdm\n",
    "x_train_essay_tfidf_w2v =[]\n",
    "for sentence in tqdm(x_train[\"essay\"].values):\n",
    "    vector = np.zeros(300) # As glove data is 300 dim\n",
    "    tfidf_weight = 0\n",
    "    for word in sentence.split():\n",
    "        if((word in glove_words) and (word in tfidf_words)):\n",
    "            # will get the vector for every single word in a sentence from glove words pickl file\n",
    "            vec = model[word] \n",
    "            # calculating tfidf value\n",
    "            tfidf = dictionary[word]*(sentence.count(word)/len(sentence.split()))\n",
    "            vector += (vec*tfidf)\n",
    "            tfidf_weight += tfidf\n",
    "    if(tfidf_weight!=0):\n",
    "        vector/=tfidf_weight\n",
    "    x_train_essay_tfidf_w2v.append(vector)\n",
    "        \n",
    "# Similarly for test data\n",
    "x_test_essay_tfidf_w2v =[]\n",
    "for sentence in tqdm(x_test[\"essay\"].values):\n",
    "    vector = np.zeros(300) # As glove data is 300 dim\n",
    "    tfidf_weight = 0\n",
    "    for word in sentence.split():\n",
    "        vector\n",
    "        if ((word in glove_words) and (word in tfidf_words)):\n",
    "            vec = model[word]\n",
    "            tfidf = dictionary[word]*(sentence.count(word)/len(sentence.split()))\n",
    "            vector += (vec*tfidf)\n",
    "            tfidf_weight += tfidf\n",
    "    if(tfidf_weight!=0):\n",
    "        vector/=tfidf_weight\n",
    "    x_test_essay_tfidf_w2v.append(vector)\n",
    "\n",
    "x_train_essay_tfidf_w2v = np.array(x_train_essay_tfidf_w2v)\n",
    "x_test_essay_tfidf_w2v = np.array(x_test_essay_tfidf_w2v)\n",
    "print(\"After TFIDF-W2V vectorization for feature: essay\")\n",
    "print(x_train_essay_tfidf_w2v.shape,y_train.shape)\n",
    "print(x_test_essay_tfidf_w2v.shape,y_test.shape)\n",
    "print(\"*\"*50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3Vxxv2Hsc31"
   },
   "source": [
    "<h2>1.4 Make Data Model Ready: encoding numerical, categorical features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RAtihVPqsc33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Categorical features ********************\n",
      "After Response coding of categorical feature: teacher_prefix\n",
      "(73196, 2) (73196,)\n",
      "(36052, 2) (36052,)\n",
      "==================================================\n",
      "After Response coding of categorical feature: project_grade_category\n",
      "(73196, 2) (73196,)\n",
      "(36052, 2) (36052,)\n",
      "==================================================\n",
      "After Response coding of categorical feature: school_state\n",
      "(73196, 2) (73196,)\n",
      "(36052, 2) (36052,)\n",
      "==================================================\n",
      "After Response coding of categorical feature: clean_categories\n",
      "(73196, 2) (73196,)\n",
      "(36052, 2) (36052,)\n",
      "==================================================\n",
      "After Response coding of categorical feature: clean_subcategories\n",
      "(73196, 2) (73196,)\n",
      "(36052, 2) (36052,)\n",
      "==================================================\n",
      "******************** Numerical features ********************\n",
      "After vectorizations of categorical feature: Price\n",
      "(73196, 1) (73196,)\n",
      "(36052, 1) (36052,)\n",
      "==================================================\n",
      "After vectorizations of categorical feature: teacher_number_of_previously_posted_projects\n",
      "(73196, 1) (73196,)\n",
      "(36052, 1) (36052,)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding \n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "    \n",
    "print(\"*\"*20,\"Categorical features\",\"*\"*20)\n",
    "# performing response encoding for categorical features\n",
    "\n",
    "# Teacher_prefix\n",
    "x_train[\"project_is_approved\"] = list(y_train) # adding y_train for checking probability values\n",
    "den = x_train[\"teacher_prefix\"].value_counts().to_dict() # for denominator of each row\n",
    "x_train[\"teacher1\"] = np.nan\n",
    "x_train[\"teacher2\"] = np.nan\n",
    "x_test[\"teacher1\"] = np.nan\n",
    "x_test[\"teacher2\"] = np.nan\n",
    "for i in x_train[\"teacher_prefix\"].unique():\n",
    "    count1 = x_train[\"teacher_prefix\"][(x_train[\"teacher_prefix\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "    count2 = x_train[\"teacher_prefix\"][(x_train[\"teacher_prefix\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "    x_train[\"teacher1\"][x_train[\"teacher_prefix\"]==i]=count1/den[i]\n",
    "    x_train[\"teacher2\"][x_train[\"teacher_prefix\"]==i]=count2/den[i]    \n",
    "\n",
    "for i in x_test[\"teacher_prefix\"].unique():\n",
    "    if i in den.keys():\n",
    "        count1 = x_train[\"teacher_prefix\"][(x_train[\"teacher_prefix\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "        count2 = x_train[\"teacher_prefix\"][(x_train[\"teacher_prefix\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "        x_test[\"teacher1\"][x_test[\"teacher_prefix\"]==i]=count1/den[i]\n",
    "        x_test[\"teacher2\"][x_test[\"teacher_prefix\"]==i]=count2/den[i]\n",
    "    else:\n",
    "        x_test[\"teacher1\"][x_test[\"teacher_prefix\"]==i]=0.5\n",
    "        x_test[\"teacher2\"][x_test[\"teacher_prefix\"]==i]=0.5\n",
    "        \n",
    "x_train_teacher_resp = np.asarray(x_train[[\"teacher1\",\"teacher2\"]]) \n",
    "x_test_teacher_resp = np.asarray(x_test[[\"teacher1\",\"teacher2\"]])\n",
    "\n",
    "print(\"After Response coding of categorical feature: teacher_prefix\")\n",
    "print(x_train_teacher_resp.shape, y_train.shape)\n",
    "print(x_test_teacher_resp.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# project_grade_category\n",
    "den = x_train[\"project_grade_category\"].value_counts().to_dict() # for denominator of each row\n",
    "x_train[\"proj1\"] = np.nan\n",
    "x_train[\"proj2\"] = np.nan\n",
    "x_test[\"proj1\"] = np.nan\n",
    "x_test[\"proj2\"] = np.nan\n",
    "for i in x_train[\"project_grade_category\"].unique():\n",
    "    count1 = x_train[\"project_grade_category\"][(x_train[\"project_grade_category\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "    count2 = x_train[\"project_grade_category\"][(x_train[\"project_grade_category\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "    x_train[\"proj1\"][x_train[\"project_grade_category\"]==i]=count1/den[i]\n",
    "    x_train[\"proj2\"][x_train[\"project_grade_category\"]==i]=count2/den[i]    \n",
    "\n",
    "for i in x_test[\"project_grade_category\"].unique():\n",
    "    if i in den.keys():\n",
    "        count1 = x_train[\"project_grade_category\"][(x_train[\"project_grade_category\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "        count2 = x_train[\"project_grade_category\"][(x_train[\"project_grade_category\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "        x_test[\"proj1\"][x_test[\"project_grade_category\"]==i]=count1/den[i]\n",
    "        x_test[\"proj2\"][x_test[\"project_grade_category\"]==i]=count2/den[i]\n",
    "    else:\n",
    "        x_test[\"proj1\"][x_test[\"project_grade_category\"]==i]=0.5\n",
    "        x_test[\"proj2\"][x_test[\"project_grade_category\"]==i]=0.5\n",
    "\n",
    "x_train_prj_grade_resp = np.asarray(x_train[[\"proj1\",\"proj2\"]]) \n",
    "x_test_prj_grade_resp = np.asarray(x_test[[\"proj1\",\"proj2\"]])\n",
    "\n",
    "print(\"After Response coding of categorical feature: project_grade_category\")\n",
    "print(x_train_prj_grade_resp.shape, y_train.shape)\n",
    "print(x_test_prj_grade_resp.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# school_state\n",
    "den = x_train[\"school_state\"].value_counts().to_dict() # for denominator of each row\n",
    "x_train[\"school1\"] = np.nan\n",
    "x_train[\"school2\"] = np.nan\n",
    "x_test[\"school1\"] = np.nan\n",
    "x_test[\"school2\"] = np.nan\n",
    "for i in x_train[\"school_state\"].unique():\n",
    "    count1 = x_train[\"school_state\"][(x_train[\"school_state\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "    count2 = x_train[\"school_state\"][(x_train[\"school_state\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "    x_train[\"school1\"][x_train[\"school_state\"]==i]=count1/den[i]\n",
    "    x_train[\"school2\"][x_train[\"school_state\"]==i]=count2/den[i]    \n",
    "\n",
    "for i in x_test[\"school_state\"].unique():\n",
    "    if i in den.keys():\n",
    "        count1 = x_train[\"school_state\"][(x_train[\"school_state\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "        count2 = x_train[\"school_state\"][(x_train[\"school_state\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "        x_test[\"school1\"][x_test[\"school_state\"]==i]=count1/den[i]\n",
    "        x_test[\"school2\"][x_test[\"school_state\"]==i]=count2/den[i]\n",
    "    else:\n",
    "        x_test[\"school1\"][x_test[\"school_state\"]==i]=0.5\n",
    "        x_test[\"school2\"][x_test[\"school_state\"]==i]=0.5\n",
    "\n",
    "x_train_school_resp = np.asarray(x_train[[\"school1\",\"school2\"]]) \n",
    "x_test_school_resp = np.asarray(x_test[[\"school1\",\"school2\"]])\n",
    "\n",
    "print(\"After Response coding of categorical feature: school_state\")\n",
    "print(x_train_school_resp.shape, y_train.shape)\n",
    "print(x_test_school_resp.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Clean categories\n",
    "den = x_train[\"clean_categories\"].value_counts().to_dict() # for denominator of each row\n",
    "x_train[\"clean1\"] = np.nan\n",
    "x_train[\"clean2\"] = np.nan\n",
    "x_test[\"clean1\"] = np.nan\n",
    "x_test[\"clean2\"] = np.nan\n",
    "for i in x_train[\"clean_categories\"].unique():\n",
    "    count1 = x_train[\"clean_categories\"][(x_train[\"clean_categories\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "    count2 = x_train[\"clean_categories\"][(x_train[\"clean_categories\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "    x_train[\"clean1\"][x_train[\"clean_categories\"]==i]=count1/den[i]\n",
    "    x_train[\"clean2\"][x_train[\"clean_categories\"]==i]=count2/den[i]    \n",
    "\n",
    "for i in x_test[\"clean_categories\"].unique():\n",
    "    if i in den.keys():\n",
    "        count1 = x_train[\"clean_categories\"][(x_train[\"clean_categories\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "        count2 = x_train[\"clean_categories\"][(x_train[\"clean_categories\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "        x_test[\"clean1\"][x_test[\"clean_categories\"]==i]=count1/den[i]\n",
    "        x_test[\"clean2\"][x_test[\"clean_categories\"]==i]=count2/den[i]\n",
    "    else:\n",
    "        x_test[\"clean1\"][x_test[\"clean_categories\"]==i]=0.5\n",
    "        x_test[\"clean2\"][x_test[\"clean_categories\"]==i]=0.5\n",
    "\n",
    "x_train_cl_categories_resp = np.asarray(x_train[[\"clean1\",\"clean2\"]]) \n",
    "x_test_cl_categories_resp = np.asarray(x_test[[\"clean1\",\"clean2\"]])\n",
    "\n",
    "print(\"After Response coding of categorical feature: clean_categories\")\n",
    "print(x_train_cl_categories_resp.shape, y_train.shape)\n",
    "print(x_test_cl_categories_resp.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Clean sub-categories\n",
    "den = x_train[\"clean_subcategories\"].value_counts().to_dict() # for denominator of each row\n",
    "x_train[\"cleansub1\"] = np.nan\n",
    "x_train[\"cleansub2\"] = np.nan\n",
    "x_test[\"cleansub1\"] = np.nan\n",
    "x_test[\"cleansub2\"] = np.nan\n",
    "for i in x_train[\"clean_subcategories\"].unique():\n",
    "    count1 = x_train[\"clean_subcategories\"][(x_train[\"clean_subcategories\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "    count2 = x_train[\"clean_subcategories\"][(x_train[\"clean_subcategories\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "    x_train[\"cleansub1\"][x_train[\"clean_subcategories\"]==i]=count1/den[i]\n",
    "    x_train[\"cleansub2\"][x_train[\"clean_subcategories\"]==i]=count2/den[i]    \n",
    "\n",
    "for i in x_test[\"clean_subcategories\"].unique():\n",
    "    if i in den.keys():\n",
    "        count1 = x_train[\"clean_subcategories\"][(x_train[\"clean_subcategories\"]==i)&(x_train[\"project_is_approved\"]==1)].count()\n",
    "        count2 = x_train[\"clean_subcategories\"][(x_train[\"clean_subcategories\"]==i)&(x_train[\"project_is_approved\"]==0)].count()\n",
    "        x_test[\"cleansub1\"][x_test[\"clean_subcategories\"]==i]=count1/den[i]\n",
    "        x_test[\"cleansub2\"][x_test[\"clean_subcategories\"]==i]=count2/den[i]\n",
    "    else:\n",
    "        x_test[\"cleansub1\"][x_test[\"clean_subcategories\"]==i]=0.5\n",
    "        x_test[\"cleansub2\"][x_test[\"clean_subcategories\"]==i]=0.5\n",
    "\n",
    "x_train_cl_subcategories_resp = np.asarray(x_train[[\"cleansub1\",\"cleansub2\"]]) \n",
    "x_test_cl_subcategories_resp = np.asarray(x_test[[\"cleansub1\",\"cleansub2\"]])\n",
    "\n",
    "print(\"After Response coding of categorical feature: clean_subcategories\")\n",
    "print(x_train_cl_subcategories_resp.shape, y_train.shape)\n",
    "print(x_test_cl_subcategories_resp.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "## Removing project_approved column from the x_train as all transformations are over ##\n",
    "x_train.drop(\"project_is_approved\",axis=1,inplace=True)\n",
    "\n",
    "print(\"*\"*20,\"Numerical features\",\"*\"*20)\n",
    "# Numerical features perform column standardzation\n",
    "# Price\n",
    "from sklearn.preprocessing import Normalizer\n",
    "nm = Normalizer()\n",
    "x_train_price_nrmlz = nm.fit_transform(x_train[\"price\"].values.reshape(1,-1))\n",
    "x_test_price_nrmlz = nm.transform(x_test[\"price\"].values.reshape(1,-1))\n",
    "x_train_price_nrmlz = x_train_price_nrmlz.reshape(-1,1)\n",
    "x_test_price_nrmlz = x_test_price_nrmlz.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations of categorical feature: Price\")\n",
    "print(x_train_price_nrmlz.shape, y_train.shape)\n",
    "print(x_test_price_nrmlz.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Number of projects previously done by teacher\n",
    "nm = Normalizer()\n",
    "x_train_num_proj_teacher_nrmlz = nm.fit_transform(x_train[\"teacher_number_of_previously_posted_projects\"].values.reshape(1,-1))\n",
    "x_test_num_proj_teacher_nrmlz = nm.transform(x_test[\"teacher_number_of_previously_posted_projects\"].values.reshape(1,-1))\n",
    "x_train_num_proj_teacher_nrmlz = x_train_num_proj_teacher_nrmlz.reshape(-1,1)\n",
    "x_test_num_proj_teacher_nrmlz = x_test_num_proj_teacher_nrmlz.reshape(-1,1)\n",
    "\n",
    "print(\"After vectorizations of categorical feature: teacher_number_of_previously_posted_projects\")\n",
    "print(x_train_num_proj_teacher_nrmlz.shape, y_train.shape)\n",
    "print(x_test_num_proj_teacher_nrmlz.shape, y_test.shape)\n",
    "print(\"=\"*50)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.5 Make Data Model Ready: Sentimental analysis on essay feature and converting into four features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73196, 4)\n",
      "(36052, 4)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "x_train_essay_senti = []\n",
    "x_test_essay_senti = []\n",
    "\n",
    "for ess in x_train[\"essay\"].values:\n",
    "    ss = sid.polarity_scores(ess)\n",
    "    x_train_essay_senti.append(list(ss.values()))\n",
    "\n",
    "for ess in x_test[\"essay\"].values:\n",
    "    ss = sid.polarity_scores(ess)\n",
    "    x_test_essay_senti.append(list(ss.values()))\n",
    "    \n",
    "x_train_essay_senti = np.array(x_train_essay_senti)\n",
    "x_test_essay_senti = np.array(x_test_essay_senti)\n",
    "\n",
    "print(x_train_essay_senti.shape)\n",
    "print(x_test_essay_senti.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data matrix for set1:\n",
      "(73196, 5016) (73196,)\n",
      "(36052, 5016) (36052,)\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (73196,300) into shape (73196)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3dafe67d7075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx_tr_tfidf_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_essay_tfidf_w2v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_teacher_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_prj_grade_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_school_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_cl_categories_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_cl_subcategories_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_price_nrmlz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_num_proj_teacher_nrmlz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train_essay_senti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mx_te_tfidf_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_essay_tfidf_w2v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_teacher_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_prj_grade_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_school_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_cl_categories_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_cl_subcategories_resp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_price_nrmlz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_num_proj_teacher_nrmlz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_essay_senti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/construct.py\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \"\"\"\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (73196,300) into shape (73196)"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "x_tr_tfidf = hstack((x_train_essay_tfidf,x_train_teacher_resp,x_train_prj_grade_resp,x_train_school_resp,x_train_cl_categories_resp,x_train_cl_subcategories_resp,x_train_price_nrmlz,x_train_num_proj_teacher_nrmlz,x_train_essay_senti)).tocsr()\n",
    "x_te_tfidf = hstack((x_test_essay_tfidf,x_test_teacher_resp,x_test_prj_grade_resp,x_test_school_resp,x_test_cl_categories_resp,x_test_cl_subcategories_resp,x_test_price_nrmlz,x_test_num_proj_teacher_nrmlz,x_test_essay_senti)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix for set1:\")\n",
    "print(x_tr_tfidf.shape, y_train.shape)\n",
    "print(x_te_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*50)\n",
    "\n",
    "x_tr_tfidf_w2v = hstack((x_train_essay_tfidf_w2v,x_train_teacher_resp,x_train_prj_grade_resp,x_train_school_resp,x_train_cl_categories_resp,x_train_cl_subcategories_resp,x_train_price_nrmlz,x_train_num_proj_teacher_nrmlz,x_train_essay_senti)).tocsr()\n",
    "x_te_tfidf_w2v = hstack((x_test_essay_tfidf_w2v,x_test_teacher_resp,x_test_prj_grade_resp,x_test_school_resp,x_test_cl_categories_resp,x_test_cl_subcategories_resp,x_test_price_nrmlz,x_test_num_proj_teacher_nrmlz,x_test_essay_senti)).tocsr() \n",
    "\n",
    "print(\"Final Data matrix for set2:\")\n",
    "print(x_tr_tfidf_w2v.shape, y_train.shape)\n",
    "print(x_te_tfidf_w2v.shape, y_test.shape)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5wDFj17sc3-"
   },
   "source": [
    "<h2>1.6 Appling Models on different kind of featurization as mentioned in the instructions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5tWox1jsc3_"
   },
   "source": [
    "<br>Apply GBDT on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARWulYUwsc4B"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhFN-lDWsc4G"
   },
   "source": [
    "<h1>3. Summary</h1>\n",
    "\n",
    "<br> as mentioned in the step 4 of instructions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11_Assignment_GBDT_Instructions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
