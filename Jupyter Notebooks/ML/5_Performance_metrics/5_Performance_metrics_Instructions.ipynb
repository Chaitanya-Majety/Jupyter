{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: [[    0     0]\n",
      " [  100 10000]]\n",
      "F1_Score: 0.9950248756218906\n",
      "accuracy_score: 0.9900990099009901\n",
      "AUC_Score: 0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "df = pd.read_csv(\"5_a.csv\")\n",
    "df[\"y_pred\"] = df[\"proba\"].apply(lambda x: 0 if x<0.5 else 1)\n",
    "# true_positives(TP)\n",
    "tp = df[\"y\"][df[\"y_pred\"]==1].where((df[\"y\"] == df[\"y_pred\"])).count()\n",
    "# true_negatives(TN)\n",
    "tn = df[\"y\"][df[\"y_pred\"]==0].where((df[\"y\"] == df[\"y_pred\"])).count()\n",
    "# False positives(FP)\n",
    "fp = df[\"y\"][df[\"y_pred\"]==1].where((df[\"y\"]== 0)).count()\n",
    "# False negatives(FN)\n",
    "fn = df[\"y\"][df[\"y_pred\"]==0].where((df[\"y\"]== 1)).count()\n",
    "# total points(n)\n",
    "n = tp+tn+fp+fn\n",
    "# confusion matrix\n",
    "cf = np.array([tn,fn,fp,tp]).reshape(2,-1)\n",
    "print(\"Confusion matrix:\",cf)\n",
    "# f1_score\n",
    "pr = tp/(fp+tp)\n",
    "re = tp/(fn+tp)\n",
    "f1_score = 2*((pr*re)/(pr+re))\n",
    "print(\"F1_Score:\",f1_score)\n",
    "# accuracy_score\n",
    "acc = (tp+tn)/n\n",
    "print(\"accuracy_score:\",acc)\n",
    "# AUC score\n",
    "df1 = df.sort_values(by=\"proba\",ignore_index=True,ascending=False)\n",
    "# df1[\"proba\"].unique() = len(df1)\n",
    "# As there are no duplicates in this csv, proceeding with direct AUC implementation\n",
    "fpr_arr = []\n",
    "tpr_arr = []\n",
    "for i in range(len(df1)):\n",
    "    temp = df1[\"proba\"][i]\n",
    "    df1[\"y_tilda\"] = df1[\"proba\"].apply(lambda x: 1 if x>=temp else 0)\n",
    "    tp1 = df1[\"y\"][df1[\"y_tilda\"]==1].where((df1[\"y\"]==1)).count()\n",
    "    fn1 = df1[\"y\"][df1[\"y_tilda\"]==0].where((df1[\"y\"]==1)).count()\n",
    "    fp1 = df1[\"y\"][df1[\"y_tilda\"]==1].where((df1[\"y\"]==0)).count()\n",
    "    tn1 = df1[\"y\"][df1[\"y_tilda\"]==0].where((df1[\"y\"]==0)).count()\n",
    "    tpr1 = (tp1)/(tp1+fn1)\n",
    "    fpr1 = (fp1)/(tn1+fp1)\n",
    "    tpr_arr.append(tpr1)\n",
    "    fpr_arr.append(fpr1)\n",
    "auc_score = np.trapz(tpr_arr,fpr_arr)\n",
    "print(\"AUC_Score:\",auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: [[9761   45]\n",
      " [ 239   55]]\n",
      "F1_Score: 0.2791878172588833\n",
      "accuracy_score: 0.9718811881188119\n",
      "AUC_Score: 0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "# write your code\n",
    "df = pd.read_csv(\"5_b.csv\")\n",
    "df[\"y_pred\"] = df[\"proba\"].apply(lambda x: 0 if x<0.5 else 1)\n",
    "# true_positives(TP)\n",
    "tp = df[\"y\"][df[\"y_pred\"]==1].where((df[\"y\"] == df[\"y_pred\"])).count()\n",
    "# true_negatives(TN)\n",
    "tn = df[\"y\"][df[\"y_pred\"]==0].where((df[\"y\"] == df[\"y_pred\"])).count()\n",
    "# False positives(FP)\n",
    "fp = df[\"y\"][df[\"y_pred\"]==1].where((df[\"y\"]== 0)).count()\n",
    "# False negatives(FN)\n",
    "fn = df[\"y\"][df[\"y_pred\"]==0].where((df[\"y\"]== 1)).count()\n",
    "# total points(n)\n",
    "n = tp+tn+fp+fn\n",
    "# confusion matrix\n",
    "cf = np.array([tn,fn,fp,tp]).reshape(2,-1)\n",
    "print(\"Confusion matrix:\",cf)\n",
    "# f1_score\n",
    "pr = tp/(fp+tp)\n",
    "re = tp/(fn+tp)\n",
    "f1_score = 2*((pr*re)/(pr+re))\n",
    "print(\"F1_Score:\",f1_score)\n",
    "# accuracy_score\n",
    "acc = (tp+tn)/n\n",
    "print(\"accuracy_score:\",acc)\n",
    "# AUC score\n",
    "df1 = df.sort_values(by=\"proba\",ignore_index=True,ascending=False)\n",
    "# df1[\"proba\"].unique() = len(df1)\n",
    "## as there are no duplicates in this csv, proceeding with direct AUC implementation\n",
    "fpr_arr = []\n",
    "tpr_arr = []\n",
    "for i in range(len(df1)):\n",
    "    temp = df1[\"proba\"][i]\n",
    "    df1[\"y_tilda\"] = df1[\"proba\"].apply(lambda x: 1 if x>=temp else 0)\n",
    "    tp1 = df1[\"y\"][df1[\"y_tilda\"]==1].where((df1[\"y\"]==1)).count()\n",
    "    fn1 = df1[\"y\"][df1[\"y_tilda\"]==0].where((df1[\"y\"]==1)).count()\n",
    "    fp1 = df1[\"y\"][df1[\"y_tilda\"]==1].where((df1[\"y\"]==0)).count()\n",
    "    tn1 = df1[\"y\"][df1[\"y_tilda\"]==0].where((df1[\"y\"]==0)).count()\n",
    "    tpr1 = (tp1)/(tp1+fn1)\n",
    "    fpr1 = (fp1)/(tn1+fp1)\n",
    "    tpr_arr.append(tpr1)\n",
    "    fpr_arr.append(fpr1)\n",
    "auc_score = np.trapz(tpr_arr,fpr_arr)\n",
    "print(\"AUC_Score:\",auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value of A is at threshold: 0.250403339798386\n"
     ]
    }
   ],
   "source": [
    " # write your code\n",
    "df = pd.read_csv(\"5_c.csv\")\n",
    "df1 = df.sort_values(by=\"prob\",ignore_index=True,ascending=False)\n",
    "# df1[\"proba\"].unique() != len(df1)\n",
    "# so need to remove the duplicates\n",
    "df1.drop_duplicates(\"prob\",keep=\"first\",inplace=True,ignore_index=True) # taking unique values alone\n",
    "arr = []\n",
    "for i in range(len(df1)):\n",
    "    temp = df1[\"prob\"][i]\n",
    "    df1[\"y_tilda\"] = df1[\"prob\"].apply(lambda x: 1 if x>=temp else 0)\n",
    "    fn1 = df1[\"y\"][df1[\"y_tilda\"]==0].where((df1[\"y\"]== 1)).count()\n",
    "    fp1 = df1[\"y\"][df1[\"y_tilda\"]==1].where((df1[\"y\"]== 0)).count()\n",
    "    A = (500*fn1) + (100*fp1) # penalising false negatives more as we are interested in positive points\n",
    "    arr.append(A)\n",
    "\n",
    "min_threshold = df1[\"prob\"][np.argmin(arr)] # threshold(probability) value where we got minimum value of A\n",
    "print(\"Minimum value of A is at threshold:\",min_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "<pre>\n",
    "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
    "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean square Error: 177.16569974554707\n",
      "Modified-Mean Absolute Percentage Error: 12.91202994009687\n",
      "R_squared_error: 0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"5_d.csv\")\n",
    "\n",
    "# Mean square error(MSE)\n",
    "summ=0\n",
    "for i in range(len(df)):\n",
    "    summ += (df[\"y\"][i] - df[\"pred\"][i])**2\n",
    "mse = (1/len(df))*summ\n",
    "print(\"Mean square Error:\",mse)\n",
    "\n",
    "# Modified - Mean absolute percentage error(Modified - MAPE)\n",
    "# Modified MAPE is used because in the given CSV data, we have y as 0 in some places\n",
    "error = 0\n",
    "actual_y_sum = np.sum(df[\"y\"])\n",
    "for i in range(len(df)):\n",
    "    error += abs(df[\"y\"][i] - df[\"pred\"][i])\n",
    "mape = (error/actual_y_sum)*100\n",
    "print(\"Modified-Mean Absolute Percentage Error:\",mape)\n",
    "\n",
    "# R-squared error\n",
    "ss_res = 0\n",
    "for i in range(len(df)):\n",
    "    ss_res += (df[\"y\"][i] - df[\"pred\"][i])**2\n",
    "mn = df[\"y\"].mean()\n",
    "ss_tot=0\n",
    "for i in range(len(df)):\n",
    "    ss_tot += (df[\"y\"][i] - mn)**2   \n",
    "r_squared = 1 - (ss_res/ss_tot)\n",
    "print(\"R_squared_error:\",r_squared) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
